{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is unsupervised learning in the contaxt of machine learning\n",
        "What is unsupervised learning in the contaxt of machine learning"
      ],
      "metadata": {
        "id": "v8Q9Mu23hLFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does K-Mean clustering algorithm work\n",
        " It works by iteratively assigning each data point to the nearest cluster centroid and then re-calculating the position of each centroid as the new mean of its assigned points until the cluster assignments no longer change."
      ],
      "metadata": {
        "id": "Y3lB-a1khQsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.expalin the concept dendogram in hierarchical clustering\n",
        "A dendrogram is a tree-like diagram used in hierarchical clustering to visualize the relationships between data points as they merge into clusters"
      ],
      "metadata": {
        "id": "y3rI3D0PhQvl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r_IMEswa5-jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the main difference between K-Means and hierarchical clustering\n",
        "The main difference is that K-Means requires you to specify the number of clusters (\\(k\\)) beforehand, while hierarchical clustering builds a hierarchy of clusters without this requirement,"
      ],
      "metadata": {
        "id": "XT4_3lq-hQzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are advantages of DBSCAN over K-Mean clustering\n",
        "DBSCAN's main advantages over K-Means are its ability to discover clusters of arbitrary shapes, handle noisy data by identifying outliers, and the fact that it does not require the number of clusters to be specified beforehand."
      ],
      "metadata": {
        "id": "hJ_NiwNIhQ2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.When would you use silhout score in clustering\n",
        " silhouette score to evaluate the quality of clustering results and to help determine the optimal number of clusters (k) when the true number of clusters is unknown"
      ],
      "metadata": {
        "id": "A67G-d4XhQ_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What are the limitations of HIerarchical clustering\n",
        "The primary limitations of hierarchical clustering are its high computational cost, lack of reversibility of decisions, and sensitivity to noise and outliers."
      ],
      "metadata": {
        "id": "eQ0qNv5whRGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Why is feature scalling is important in clustering algorithm like K-Means\n",
        "Feature scaling is crucial for K-Means because the algorithm is distance-based and a feature with a larger range will disproportionately influence the distance calculations, leading to biased results"
      ],
      "metadata": {
        "id": "cCHvPymBhRLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.How does DBSCAN identify noise points\n",
        "DBSCAN identifies noise points by first labeling core points"
      ],
      "metadata": {
        "id": "pRA1qVtzhRP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Define inertia in the context of K-means\n",
        "In the context of K-means clustering, inertia (also known as the within-cluster sum of squares (WCSS)) is a metric that measures the compactness or internal coherence of the clusters"
      ],
      "metadata": {
        "id": "DhtXEugphRVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 11. What is the elbow method in K-means clustering\n",
        ""
      ],
      "metadata": {
        "id": "eP4ihVx3hSXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Describe th econcept of density in DBSCAN\n",
        "In DBSCAN, density is defined as a region containing a high number of data points within a specified distance (\n",
        ") from each other. The algorithm identifies clusters as dense regions separated by sparser areas, which it labels as noise.\n"
      ],
      "metadata": {
        "id": "E4rlcx8PhSaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Can hierarchical clustering be used on categorical data\n",
        "yes"
      ],
      "metadata": {
        "id": "0lfgjn1GhSgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What does a negative silhout score indicate\n",
        "a sample has been assigned to the wrong cluster"
      ],
      "metadata": {
        "id": "1UYBpigOhSkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Explain the term linkage criteria in hierarchical clustering\n",
        " rules that determine how to measure the distance between clusters, guiding the algorithm in merging or splitting them."
      ],
      "metadata": {
        "id": "L2-tfv7KhSob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Why might K-means clustering perform poorly on data wit varying cluster\n",
        "size or density\n",
        "because its objective function, based on minimizing the sum of squared distances from the centroid, inherently assumes that clusters are spherical, roughly equal in size, and have similar variance (density) [1, 2]."
      ],
      "metadata": {
        "id": "wAva-z_JhSsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are the core parameter in DBSCAN and how do they influence\n",
        "clustering\n",
        "algorithm are \\(\\epsilon \\) (epsilon or eps) and MinPts,"
      ],
      "metadata": {
        "id": "_SVZUxn9hSx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.How does K-means ++ improve upon standard K-means initializes\n",
        "using a smarter, distance-based method for initializing centroids"
      ],
      "metadata": {
        "id": "6r5E099yhS07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What is algomerative clustering\n",
        "a bottom-up, hierarchical clustering method that starts with each data point as its own cluster and iteratively merges the closest pairs of clusters until only one cluster remains"
      ],
      "metadata": {
        "id": "lTGnOQ7PhS4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What makes Silhoutte score a better metric than just inertia for model evaluation\n",
        "because it provides a more comprehensive evaluation of cluster quality by measuring both the cohesion (how well points are grouped within a cluster) and the separation (how distinct clusters are from one another)"
      ],
      "metadata": {
        "id": "YCC9VItkhS8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL\n"
      ],
      "metadata": {
        "id": "Rt7IC--ik_8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21 generate syntehtic data with 4 centers using make_blolbs and apply K-means clustering visualize using a scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate synthetic data with 4 centers\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# 2. Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=0, n_init=10) # n_init is added for clarity and to suppress a future warning\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "# 3. Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.7)\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.9, marker='X', label='Centroids')\n",
        "\n",
        "plt.title('K-means Clustering with 4 Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QsvST8yZlSLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Load the iris dataset and use Agglamerative clustering to group thw data into 3 clusters display the first 10 predicted table\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "iris_df = pd.DataFrame(data=X, columns=feature_names)\n",
        "\n",
        "# Apply Agglomerative Clustering with 3 clusters\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward') # 'ward' minimizes variance within clusters\n",
        "predicted_clusters = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Add the predicted clusters to the DataFrame\n",
        "iris_df['predicted_cluster'] = predicted_clusters\n",
        "\n",
        "# Display the first 10 rows with predicted clusters\n",
        "print(\"First 10 rows of the Iris dataset with predicted clusters:\")\n",
        "print(iris_df.head(10))"
      ],
      "metadata": {
        "id": "GTZx_2-OlTU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 generate synthetic data using make_moons and apply DBSCAN Highlight outliers in the plot"
      ],
      "metadata": {
        "id": "jrbKaTG3lTYz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# 1. Generate synthetic data using make_moons\n",
        "X, y = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
        "\n",
        "# 2. Apply DBSCAN clustering\n",
        "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X)\n",
        "\n",
        "# 3. Visualize the results, highlighting outliers\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot core points and border points belonging to clusters\n",
        "unique_clusters = set(clusters)\n",
        "colors = plt.cm.Spectral(float(i) / len(unique_clusters) for i in unique_clusters)\n",
        "for i, col in zip(unique_clusters, colors):\n",
        "    if i == -1:  # Outliers are labeled as -1 by DBSCAN\n",
        "        # Plot outliers in black\n",
        "        plt.scatter(X[clusters == i, 0], X[clusters == i, 1],\n",
        "                    s=50, c='black', marker='x', label='Outliers')\n",
        "    else:\n",
        "        # Plot cluster points\n",
        "        plt.scatter(X[clusters == i, 0], X[clusters == i, 1],\n",
        "                    s=50, c=col, marker='o', label=f'Cluster {i}')\n",
        "\n",
        "plt.title('DBSCAN Clustering on Make_Moons Data with Outliers Highlighted')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()#24 Load the Wine dataset and apply Minm=MaxScaler and K-means with 2 clusters Output the cluster centerooids"
      ],
      "metadata": {
        "id": "t9tlax01lTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Use make_circles to generate synthetic data and cluster it using DBSCAN plot the result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate synthetic data using make_circles\n",
        "X, y = make_circles(n_samples=750, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# 2. Apply DBSCAN clustering\n",
        "# Adjust eps and min_samples based on the noise and density of your data\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X)\n",
        "\n",
        "# 3. Plot the results\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Get unique cluster labels, including -1 for noise\n",
        "unique_labels = np.unique(clusters)\n",
        "\n",
        "# Define a color map for clusters\n",
        "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:  # Noise points are colored black\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = (clusters == k)\n",
        "    xy = X[class_member_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], c=[col], s=50, label=f'Cluster {k}' if k != -1 else 'Noise')\n",
        "\n",
        "plt.title('DBSCAN Clustering on Make_Circles Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NwznwlkGlTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 load the breast cancer dataset appply MinMaxScaler  and use K-means with 2 cluster output the cluster centeroids\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# 2. Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Use K-means with 2 clusters\n",
        "# Set a random state for reproducibility\n",
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# 4. Get the cluster centroids\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Convert centroids to a pandas DataFrame for better readability, using feature names\n",
        "centroids_df = pd.DataFrame(centroids, columns=feature_names)\n",
        "centroids_df.index = ['Cluster 1', 'Cluster 2']\n",
        "\n",
        "# Output the cluster centroids\n",
        "print(\"Cluster Centroids (scaled values):\")\n",
        "print(centroids_df)\n"
      ],
      "metadata": {
        "id": "Nk9WqiQNlTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 generate synthetic data using make_blolbs with varying clusters standard deviation and cluster with DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate synthetic data with varying cluster standard deviations\n",
        "n_samples = 1500\n",
        "random_state = 170\n",
        "X, y = make_blobs(n_samples=n_samples, random_state=random_state,\n",
        "                  centers=[[0,0], [3,3], [-3,3]],  # Example cluster centers\n",
        "                  cluster_std=[0.5, 1.5, 0.7])  # Varying standard deviations\n",
        "\n",
        "# 2. Apply DBSCAN clustering\n",
        "# You might need to tune eps and min_samples based on your data and desired cluster characteristics.\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X)\n",
        "\n",
        "# 3. Visualize the results\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot the original data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.8)\n",
        "\n",
        "# Highlight noise points (assigned cluster label -1 by DBSCAN)\n",
        "noise_points = X[clusters == -1]\n",
        "plt.scatter(noise_points[:, 0], noise_points[:, 1], c='red', marker='x', s=100, label='Noise')\n",
        "\n",
        "plt.title('DBSCAN Clustering on make_blobs with Varying Cluster Standard Deviations')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ZOJgD52lTnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 load the digits dataset reduce it to 2d using PCA and visualize cluster from K-means\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Load the digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target # True labels for comparison/evaluation\n",
        "\n",
        "# 2. Reduce the dataset to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# 3. Apply K-Means clustering to the reduced data\n",
        "# We know there are 10 digits (0-9), so we'll set n_clusters=10\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10) # n_init to avoid local minima\n",
        "clusters = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# 4. Visualize the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Cluster Centers')\n",
        "plt.title('K-Means Clustering of Digits Dataset (PCA Reduced to 2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-i100WclTra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29Creat synthetic data make_blobs  and evaluate silhouette score for k=2 to 5 display as a bar chart\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# 1. Create synthetic data\n",
        "# Generating data with 500 samples, 2 features, and 4 actual centers for a good test case\n",
        "X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1,\n",
        "                  shuffle=True, random_state=1)\n",
        "\n",
        "# 2. Evaluate silhouette score for k=2 to 5\n",
        "range_n_clusters = [2, 3, 4, 5]\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Initialize KMeans model and fit to the data\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "    # Calculate the average silhouette score\n",
        "    score = silhouette_score(X, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is: {score}\")\n",
        "\n",
        "# 3. Display as a bar chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(range_n_clusters, silhouette_scores, tick_label=range_n_clusters, color='skyblue')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Average Silhouette Score')\n",
        "plt.title('Silhouette Scores for different K values')\n",
        "plt.ylim(0, 1) # Silhouette scores range from -1 to 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2C1fvTRslTvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 load the iris dataset and use heirarchical clustering to group data plot a dindogram with average linkage\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features of the Iris dataset\n",
        "\n",
        "# Perform hierarchical clustering with average linkage\n",
        "# 'average' linkage defines the distance between two clusters as the average distance\n",
        "# between each point in one cluster and every point in the other cluster.\n",
        "linked = linkage(X, method='average')\n",
        "\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Average Linkage) - Iris Dataset')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ti-b2F9SlTy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31 generate synthetic data with overlapping clusters using make_blobs the apply K_means and visualize ith decision bondaries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# 1. Generate synthetic data with overlapping clusters\n",
        "# Using a high cluster_std value (e.g., 2.0) creates significant overlap\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=2.0, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# 3. Visualize with decision boundaries\n",
        "# Create a meshgrid to cover the entire feature space\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict the cluster for each point in the meshgrid\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Define color maps\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#FAEBD7'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#DAA520'])\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot the decision boundaries\n",
        "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.6)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=cmap_bold, s=50, edgecolor='k')\n",
        "\n",
        "# Plot the cluster centers\n",
        "plt.scatter(centers[:, 0], centers[:, 1], marker='o', c='black', s=200, alpha=0.7, label='Centroids')\n",
        "\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.title(\"K-Means Clustering with Decision Boundaries on Overlapping Blobs\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dmxmRqNKlT2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32 load the digit dataset and apply DBSCAN after reducing dimensions with t_SNE visualize the result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the digit dataset\n",
        "digits = load_digits()\n",
        "X = digits.data  # Features (64-dimensional pixel data)\n",
        "y = digits.target # True labels of the digits\n",
        "\n",
        "# 2. Standardize the data (important for t-SNE and DBSCAN)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply t-SNE for dimensionality reduction to 2 dimensions\n",
        "# Adjust perplexity and n_iter for optimal visualization\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# 4. Apply DBSCAN clustering on the t-SNE reduced data\n",
        "# Determine optimal eps and min_samples through methods like k-distance plot or experimentation\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5) # Example values, adjust as needed\n",
        "clusters = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# 5. Visualize the result\n",
        "plt.figure(figsize=(10, 8))\n",
        "# Plot points colored by DBSCAN clusters\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='viridis', s=20, alpha=0.7)\n",
        "plt.title('DBSCAN Clustering on t-SNE Reduced Digits Dataset')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(label='DBSCAN Cluster Label')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Visualize with true labels to compare clustering performance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=20, alpha=0.7)\n",
        "plt.title('t-SNE Visualization of Digits Dataset (True Labels)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(label='True Digit Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ONVPTjnklT6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33 generate synthetic data using make_blobs and apply aglomerative clustering with complete linkage plot the result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# 1. Generate synthetic data using make_blobs\n",
        "n_samples = 300\n",
        "n_features = 2\n",
        "n_centers = 4\n",
        "cluster_std = 1.0\n",
        "random_state = 42\n",
        "\n",
        "X, y_true = make_blobs(n_samples=n_samples, n_features=n_features,\n",
        "                       centers=n_centers, cluster_std=cluster_std,\n",
        "                       random_state=random_state)\n",
        "\n",
        "# 2. Apply Agglomerative Clustering with complete linkage\n",
        "n_clusters = n_centers  # Assuming we want to find the original number of clusters\n",
        "agglomerative = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\n",
        "labels = agglomerative.fit_predict(X)\n",
        "\n",
        "# 3. Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the original data (for comparison, optional)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.title('Original Data (True Clusters)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Plot the clustered data\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
        "plt.title(f'Agglomerative Clustering (Complete Linkage, {n_clusters} Clusters)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MxzeF-SVlT-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34 load the breast cancer dataset compare inertia values for k=2 to 6 using k-means show result in a plot\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# (Code to calculate inertia values as above)\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "inertia_values = []\n",
        "k_values = range(2, 7)\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, inertia_values, marker='o')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_css6P0lUB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35 generate synthetic concentric using make_circles and cluster using Agglomerative clustering with single linkage\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate synthetic concentric circle data\n",
        "# n_samples: total number of points\n",
        "# noise: standard deviation of Gaussian noise\n",
        "# factor: scale factor between inner and outer circle\n",
        "X, y = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "# 2. Apply Agglomerative Clustering with single linkage\n",
        "# n_clusters: the number of clusters to form (here, 2 concentric circles)\n",
        "# linkage: the linkage criterion to use (here, 'single')\n",
        "# metric: the distance metric (here, 'euclidean' as default is fine for single linkage)\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='single', metric='euclidean')\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# 3. Visualize the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot original data (optional, for comparison)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
        "plt.title('Original Data (True Labels)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Plot clustered data\n",
        "plt.subplot(1, 2, 2)\n",
        "# The 'labels' variable contains the cluster assignments from Agglomerative Clustering\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
        "plt.title('Agglomerative Clustering (Single Linkage)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lyMcljjtlUGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36 use the wine dataset apply DBSCAN aftter scaling the data and the count data number of cluster (excluding noise )\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# 1. Load the Wine Dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# 2. Scale the Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Apply DBSCAN\n",
        "# You may need to tune eps and min_samples for optimal results\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# 4. Count Clusters (Excluding Noise)\n",
        "# Filter out noise points (labeled -1)\n",
        "valid_clusters = clusters[clusters != -1]\n",
        "num_clusters = len(np.unique(valid_clusters))\n",
        "\n",
        "print(f\"Number of clusters found (excluding noise): {num_clusters}\")"
      ],
      "metadata": {
        "id": "cRvONQj1lUK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37 generate synthetic data with make_blobs and apply k_means then plot the cluster center on top of the data point\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "# X contains the data points, y_true contains the true cluster labels (useful for comparison, but not used in K-Means fitting)\n",
        "X, y_true = make_blobs(\n",
        "    n_samples=300,\n",
        "    centers=4,\n",
        "    cluster_std=0.60,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "# We specify the number of clusters we expect (4, matching the data generation)\n",
        "kmeans = KMeans(n_clusters=4, random_state=0, n_init=10) # n_init is set to 10 for consistency with older examples, default is 'auto' in recent versions\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X) # Predict the cluster labels for the data points\n",
        "centers = kmeans.cluster_centers_ # Get the coordinates of the cluster centers\n",
        "\n",
        "# 3. Plot the data points and cluster centers\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the data points, colored by their assigned cluster labels\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', label='Data points')\n",
        "\n",
        "# Plot the cluster centers (centroids)\n",
        "plt.scatter(\n",
        "    centers[:, 0],\n",
        "    centers[:, 1],\n",
        "    c='red',\n",
        "    s=250,\n",
        "    marker='*',\n",
        "    edgecolor='black',\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "plt.title('K-Means Clustering with Cluster Centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NdiVuBmilUPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38 load the iris dataset cluster with DBSCAN and print how many samples were identified as noise\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "# You may need to tune eps and min_samples for optimal results,\n",
        "# as these parameters significantly impact the clustering.\n",
        "# For example, using eps=0.5 and min_samples=5\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Count the number of noise samples\n",
        "# In DBSCAN, noise samples are labeled as -1\n",
        "noise_samples = np.sum(labels == -1)\n",
        "\n",
        "# Print the number of noise samples\n",
        "print(f\"Number of samples identified as noise by DBSCAN: {noise_samples}\")"
      ],
      "metadata": {
        "id": "fR1iR8wClUTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39 generate synthetic non_linearly separable data using make_moons apply k_means and visualize the clustering result\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate synthetic non-linearly separable data\n",
        "X, y_true = make_moons(n_samples=200, noise=0.1, random_state=42) #\n",
        "\n",
        "# 2. Apply K-Means clustering\n",
        "# We know there are 2 underlying clusters, so we set n_clusters=2\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10) #\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X) #\n",
        "centroids = kmeans.cluster_centers_ #\n",
        "\n",
        "# 3. Visualize the clustering result\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot the original data with true labels for comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, s=50, cmap='viridis')\n",
        "plt.title('Original Data (True Labels)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Plot the data with K-Means assigned labels and centroids\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "# Plot the centroids as black 'X' marks\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='X', alpha=0.8)\n",
        "plt.title('K-Means Clustering Result')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5k1_6_K5lUXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40 load the digit dataset apply PCA to reduce to 3 componenet then use k_means and visualize with a 3D scatter plot\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Load the digit dataset\n",
        "digits = datasets.load_digits()\n",
        "X = digits.data\n",
        "y = digits.target # The true labels for comparison, not used for K-means training\n",
        "\n",
        "print(f\"Original data shape: {X.shape}\") # Original data has 64 features\n",
        "\n",
        "# 2. Apply PCA to reduce to 3 components\n",
        "pca = PCA(n_components=3)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "print(f\"Reduced data shape: {X_reduced.shape}\")\n",
        "\n",
        "# 3. Use K-means on the reduced data\n",
        "# The digits dataset has 10 classes (0-9), so we set n_clusters=10\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "kmeans.fit(X_reduced)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# 4. Visualize with a 3D scatter plot\n",
        "fig = plt.figure(1, figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=clusters, cmap='viridis', edgecolor='k', s=40)\n",
        "\n",
        "ax.set_title(\"K-Means Clustering on Digits Dataset (PCA Reduced to 3 Components)\")\n",
        "ax.set_xlabel(\"First Principal Component (PC1)\")\n",
        "ax.set_ylabel(\"Second Principal Component (PC2)\")\n",
        "ax.set_zlabel(\"Third Principal Component (PC3)\")\n",
        "ax.legend() # Optional, but helps identify clusters if a color map is assigned clearly\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MKKvJo_mlUcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41 generate synthetic blobs with 5 centers and apply k_means then use silhouette_score to evaluet the clustering\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Generate synthetic blobs with 5 centers\n",
        "n_samples = 500\n",
        "n_features = 2 # 2 features for easy visualization\n",
        "n_centers = 5\n",
        "X, y_true = make_blobs(n_samples=n_samples, centers=n_centers, n_features=n_features,\n",
        "                       cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Optional: Visualize the generated data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50, c=y_true, cmap='viridis')\n",
        "plt.title(f'Synthetic data with {n_centers} centers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "# 2. Apply K-Means clustering (using 5 clusters)\n",
        "kmeans = KMeans(n_clusters=n_centers, init='k-means++', n_init=10, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Use silhouette score to evaluate the clustering\n",
        "# The silhouette score ranges from -1 to 1, with values closer to 1 indicating better separation\n",
        "silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "\n",
        "print(f\"For n_clusters = {n_centers}, the average silhouette score is: {silhouette_avg:.4f}\")\n",
        "\n",
        "# Optional: Visualize the clustered data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', edgecolors='black')\n",
        "plt.title(f'K-Means Clustering with {n_centers} centers (Silhouette Score: {silhouette_avg:.4f})')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "edxGwbUSlUhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42 load the breast cancer dataset reduce dimensionality using PCA and apply algomerative clustering visualize in 2D\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "target_names = cancer.target_names # malignant (0), benign (1)\n",
        "\n",
        "# 2. Standardize the data\n",
        "# PCA is sensitive to scale, so standardization is crucial\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Reduce dimensionality using PCA to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original shape: {X.shape}\")\n",
        "print(f\"Reduced shape after PCA: {X_pca.shape}\")\n",
        "print(f\"Explained variance ratio of the first two components: {pca.explained_variance_ratio_.sum():.2f}\")\n",
        "\n",
        "# 4. Apply Agglomerative Clustering\n",
        "# We use 2 clusters as there are two known classes (malignant/benign) in the dataset\n",
        "# Ward linkage is often a good choice for general-purpose clustering with Euclidean distances\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
        "# fit_predict returns the cluster labels for each data point\n",
        "clusters = agg_clustering.fit_predict(X_pca)\n",
        "\n",
        "# 5. Visualize in 2D\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Define colors and markers for visualization\n",
        "colors = ['red', 'blue']\n",
        "target_labels = ['malignant', 'benign']\n",
        "\n",
        "# Scatter plot of the two principal components, colored by the *assigned clusters*\n",
        "for i, color in enumerate(colors):\n",
        "    # Select data points belonging to cluster i\n",
        "    plt.scatter(X_pca[clusters == i, 0], X_pca[clusters == i, 1],\n",
        "                c=color, label=f'Cluster {i} ({target_labels[i]})',\n",
        "                edgecolor='k', s=50)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Agglomerative Clustering (PCA-reduced Breast Cancer Data)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R3UoXJPwlUlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43 generate the noisy curcular data using make_circles and visualize clustering result from clustering  from k_,eans and DBSCAn side-by-side\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Generate noisy circular data\n",
        "n_samples = 1000\n",
        "factor = 0.5  # Controls the distance between inner and outer circles\n",
        "noise = 0.05  # Standard deviation of Gaussian noise\n",
        "X, _ = make_circles(n_samples=n_samples, factor=factor, noise=noise, random_state=42)\n",
        "\n",
        "# 2. Apply K-Means Clustering\n",
        "# K-Means struggles with non-globular shapes like circles, but we'll demonstrate it.\n",
        "# We expect 2 clusters for the two circles.\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Apply DBSCAN Clustering\n",
        "# DBSCAN is well-suited for finding clusters of arbitrary shapes and identifying noise.\n",
        "# Scaling the data can sometimes improve DBSCAN performance, but for make_circles, it's often not strictly necessary.\n",
        "# However, it's good practice for real-world datasets.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Tune eps and min_samples for DBSCAN based on the data's density.\n",
        "# For make_circles with noise=0.05, these values often work well.\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled) # Use scaled data for DBSCAN\n",
        "\n",
        "# 4. Visualize the clustering results side-by-side\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# K-Means plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.8)\n",
        "plt.title('K-Means Clustering on Noisy Circles')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# DBSCAN plot\n",
        "plt.subplot(1, 2, 2)\n",
        "# Points labeled -1 by DBSCAN are considered noise and are plotted in black.\n",
        "unique_dbscan_labels = set(dbscan_labels)\n",
        "colors = plt.cm.viridis(dbscan_labels / (len(unique_dbscan_labels) - 1))\n",
        "colors[dbscan_labels == -1] = [0, 0, 0, 1]  # Set noise points to black\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.8)\n",
        "plt.title('DBSCAN Clustering on Noisy Circles (Noise in Black)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lmcAv9FqlUp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44 load the iris dataset  plot the silhouette cosfficient for each sample after clustering\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n"
      ],
      "metadata": {
        "id": "dw5bxLJzlUuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45 generate synthetic data using make_blobs  and apply algomerative clustering using average linkage visualize cluatering\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# 1. Generate synthetic data using make_blobs\n",
        "n_samples = 300\n",
        "n_centers = 3\n",
        "# X contains the features, y contains the true labels (for evaluation/visualization)\n",
        "X, y_true = make_blobs(n_samples=n_samples, centers=n_centers, cluster_std=0.8, random_state=42)\n",
        "\n",
        "# 2. Apply Agglomerative Clustering using average linkage\n",
        "# We specify the number of clusters and the linkage criterion\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=n_centers, linkage='average')\n",
        "y_pred = agg_clustering.fit_predict(X)\n",
        "\n",
        "# 3. Visualize the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot the true clusters\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_true, s=40, cmap='viridis')\n",
        "plt.title(\"Original Data (True Clusters)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# Plot the agglomerative clustering results\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=40, cmap='viridis')\n",
        "plt.title(\"Agglomerative Clustering (Average Linkage)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oQsZaCWtlUz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46 load the wine dataset apply k_means and visualize the clustering assignment in a seaborn pairplot (first 4 feature)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the wine dataset\n",
        "wine = datasets.load_wine(as_frame=True) # Load as a pandas DataFrame\n",
        "df = wine['frame']\n",
        "# The 'target' column contains the original labels, which we will not use for clustering itself.\n",
        "\n",
        "# 2. Select the features for clustering and visualization (first 4 features)\n",
        "# The feature names are: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash']\n",
        "features = df.iloc[:, :4]\n",
        "feature_names = features.columns\n",
        "\n",
        "# Optional: Scale the data. K-means is sensitive to the scale of features.\n",
        "# We fit and transform the features data\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "features_scaled_df = pd.DataFrame(features_scaled, columns=feature_names)\n",
        "\n",
        "\n",
        "# 3. Apply K-means clustering\n",
        "# The original dataset has 3 classes, so we set n_clusters=3\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42, n_init=10)\n",
        "kmeans.fit(features_scaled_df)\n",
        "cluster_assignments = kmeans.labels_\n",
        "\n",
        "# 4. Add the cluster assignments to a new DataFrame for visualization\n",
        "# Use the unscaled features for better interpretability in the plot axes\n",
        "plot_df = features.copy()\n",
        "plot_df['Cluster'] = cluster_assignments\n",
        "# Convert the cluster column to a categorical type for better visualization\n",
        "plot_df['Cluster'] = plot_df['Cluster'].astype('category')\n",
        "\n",
        "\n",
        "# 5. Visualize the clustering assignment in a seaborn pairplot\n",
        "sns.pairplot(plot_df, hue='Cluster', palette='tab10', vars=feature_names)\n",
        "plt.suptitle('K-Means Clustering on the first 4 Wine Features', y=1.02) # Adjust title position\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qY_zkJwZlU4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47 generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points print the count\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate noisy blobs\n",
        "# n_samples: total number of points\n",
        "# centers: number of clusters\n",
        "# cluster_std: standard deviation of the clusters (controls noise level)\n",
        "# random_state: for reproducibility\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.8, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Count clusters and noise points\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise label (-1)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f\"Number of estimated clusters: {n_clusters}\")\n",
        "print(f\"Number of noise points: {n_noise}\")"
      ],
      "metadata": {
        "id": "tFr8-dmTlU9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#48 load the digit dataset reduce dimenions using t_SNE then apply agglo;merative clustering and plot the clusters\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# 1. Load the digit dataset\n",
        "digits = load_digits()\n",
        "X = digits.data  # The image data\n",
        "y = digits.target # The true labels of the digits\n",
        "\n",
        "# 2. Reduce dimensions using t-SNE\n",
        "# It is often beneficial to perform PCA as a preprocessing step for t-SNE on high-dimensional data\n",
        "# to reduce noise and speed up computation, but for simplicity, we'll directly apply t-SNE here.\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300) # Adjust perplexity and n_iter as needed\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# 3. Apply Agglomerative Clustering\n",
        "# We'll assume we know the number of clusters (10 for digits 0-9)\n",
        "n_clusters = 10\n",
        "agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "clusters = agglomerative.fit_predict(X_tsne)\n",
        "\n",
        "# 4. Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=clusters, cmap='viridis', s=10, alpha=0.7)\n",
        "plt.title('Agglomerative Clustering of Digits after t-SNE Dimensionality Reduction')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(scatter, ticks=range(n_clusters), label='Cluster Label')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Plotting with true labels for comparison\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter_true = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)\n",
        "plt.title('True Labels of Digits after t-SNE Dimensionality Reduction')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.colorbar(scatter_true, ticks=range(n_clusters), label='True Digit Label')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zM16fsy-lVC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}